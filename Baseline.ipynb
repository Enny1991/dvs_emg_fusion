{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Add, Concatenate, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from scipy.signal import stft\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle as pkl\n",
    "\n",
    "from utils import Person, analyze\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes = ['pinky', 'elle', 'yo', 'index', 'thumb']\n",
    "classes_dict = {'pinky': 0, 'elle': 1, 'yo': 2, 'index': 3, 'thumb': 4}\n",
    "classes_inv = {v: k for k, v in classes_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47745\n",
      "(47745,)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[1, 2, 3]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n"
     ]
    }
   ],
   "source": [
    "# Load image dataset\n",
    "crop_path = './dump/img_cropped/'\n",
    "all_frames = [i for i in sorted(os.listdir(crop_path)) if 'tiff' in i]\n",
    "Y_IMG = np.array([classes_dict[i.split('_')[2]] for i in sorted(os.listdir(crop_path)) if 'tiff' in i])\n",
    "SUB_IMG = np.array([int(i[7:9]) for i in sorted(os.listdir(crop_path)) if 'tiff' in i])\n",
    "SES_IMG = np.array([int(i[17:19]) for i in sorted(os.listdir(crop_path)) if 'tiff' in i])\n",
    "TRI_IMG = np.array([int(i.split('_')[3]) for i in sorted(os.listdir(crop_path)) if 'tiff' in i])\n",
    "IDX_IMG = np.array([int(i.split('_')[4].split('.')[0]) for i in sorted(os.listdir(crop_path)) if 'tiff' in i])\n",
    "\n",
    "IDX_IMG = np.array(IDX_IMG)\n",
    "Y_IMG = np.array(Y_IMG)\n",
    "SUB_IMG = np.array(SUB_IMG)\n",
    "SES_IMG = np.array(SES_IMG)\n",
    "TRI_IMG = np.array(TRI_IMG)\n",
    "\n",
    "print(len(IDX_IMG))\n",
    "print(Y_IMG.shape)\n",
    "print(list(set(SUB_IMG)))\n",
    "print(list(set(SES_IMG)))\n",
    "print(list(set(TRI_IMG)))\n",
    "print(list(set(IDX_IMG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47745, 40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "X_IMG = np.array([plt.imread(crop_path + f)[::-1] for f in all_frames])\n",
    "X_IMG = np.array(X_IMG)\n",
    "print(X_IMG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTION DICTIONARY for synchronization and allignment \n",
    "corrections = {}\n",
    "corrections['subject01_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-15, 'cy':20}\n",
    "corrections['subject01_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':0}\n",
    "corrections['subject01_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':10}\n",
    "corrections['subject02_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-40, 'cy':5}\n",
    "corrections['subject02_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':5}\n",
    "\n",
    "corrections['subject03_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject03_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject03_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject04_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-30, 'cy':-15}\n",
    "corrections['subject04_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-50, 'cy':-10}\n",
    "corrections['subject04_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-40, 'cy':-10}\n",
    "corrections['subject05_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':-10}\n",
    "corrections['subject05_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':10, 'cy':-10}\n",
    "corrections['subject05_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':-10}\n",
    "corrections['subject06_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':0}\n",
    "corrections['subject06_session02'] = {'fs':197, 'tg':'base', 'shift':1.0, 'cx':5, 'cy':-5}\n",
    "corrections['subject06_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':0}\n",
    "corrections['subject07_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject07_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':0}\n",
    "corrections['subject07_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':0}\n",
    "corrections['subject08_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-20, 'cy':10}\n",
    "corrections['subject08_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':0}\n",
    "corrections['subject08_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':20, 'cy':0}\n",
    "corrections['subject09_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "\n",
    "corrections['subject09_session02'] = {'fs':185, 'tg':'base', 'shift':2.0, 'cx':-10, 'cy':-10}\n",
    "\n",
    "corrections['subject09_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-10, 'cy':-20}\n",
    "corrections['subject10_session01'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject10_session02'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "corrections['subject10_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':0, 'cy':-10}\n",
    "\n",
    "corrections['subject11_session01'] = {'fs':174, 'tg':'base', 'shift':-4, 'cx':-5, 'cy':10}\n",
    "corrections['subject11_session02'] = {'fs':169, 'tg':'base', 'shift':-3, 'cx':-5, 'cy':15}\n",
    "corrections['subject11_session03'] = {'fs':173, 'tg':'base', 'shift':-3.5, 'cx':-8, 'cy':10}\n",
    "corrections['subject12_session01'] = {'fs':200, 'tg':'base', 'shift':-2, 'cx':0, 'cy':10}\n",
    "corrections['subject12_session02'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':0, 'cy':10}\n",
    "corrections['subject12_session03'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':0, 'cy':10}\n",
    "corrections['subject13_session01'] = {'fs':200, 'tg':'single', 'shift':-0.5, 'cx':-10, 'cy':20}\n",
    "corrections['subject13_session02'] = {'fs':200, 'tg':'base', 'shift':-3, 'cx':5, 'cy':15}\n",
    "corrections['subject13_session03'] = {'fs':200, 'tg':'base', 'shift':0, 'cx':0, 'cy':10}\n",
    "corrections['subject14_session01'] = {'fs':200, 'tg':'base', 'shift':-2, 'cx':0, 'cy':5}\n",
    "corrections['subject14_session02'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':0, 'cy':5}\n",
    "corrections['subject14_session03'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':0, 'cy':8}\n",
    "corrections['subject15_session01'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':-5, 'cy':15}\n",
    "corrections['subject15_session02'] = {'fs':200, 'tg':'base', 'shift':-1.5, 'cx':0, 'cy':15}\n",
    "corrections['subject15_session03'] = {'fs':200, 'tg':'base', 'shift':-2.5, 'cx':-5, 'cy':15}\n",
    "corrections['subject16_session01'] = {'fs':200, 'tg':'base', 'shift':-1, 'cx':-10, 'cy':15}\n",
    "corrections['subject16_session02'] = {'fs':200, 'tg':'base', 'shift':-2, 'cx':-15, 'cy':15}\n",
    "corrections['subject16_session03'] = {'fs':200, 'tg':'base', 'shift':-2, 'cx':-20, 'cy':15}\n",
    "\n",
    "corrections['subject17_session01'] = {'fs':200, 'tg':'base', 'shift':-3, 'cx':7, 'cy':10}\n",
    "corrections['subject17_session02'] = {'fs':200, 'tg':'first', 'shift':-3, 'cx':10, 'cy':10}\n",
    "corrections['subject17_session03'] = {'fs':200, 'tg':'base', 'shift':-3.2, 'cx':15, 'cy':15}\n",
    "corrections['subject18_session01'] = {'fs':200, 'tg':'base', 'shift':-4.3, 'cx':-3, 'cy':-5}\n",
    "corrections['subject18_session02'] = {'fs':200, 'tg':'base', 'shift':-3.2, 'cx':-10, 'cy':-15}\n",
    "corrections['subject18_session03'] = {'fs':200, 'tg':'base', 'shift':-4.3, 'cx':-10, 'cy':-5}\n",
    "corrections['subject19_session01'] = {'fs':167, 'tg':'base', 'shift':-2, 'cx':0, 'cy':20}\n",
    "corrections['subject19_session02'] = {'fs':173, 'tg':'base', 'shift':-2.5, 'cx':-3, 'cy':15}\n",
    "corrections['subject19_session03'] = {'fs':200, 'tg':'base', 'shift':-2, 'cx':-3, 'cy':15}\n",
    "corrections['subject20_session01'] = {'fs':174, 'tg':'base', 'shift':-2.6, 'cx':-10, 'cy':10}\n",
    "corrections['subject20_session02'] = {'fs':176, 'tg':'base', 'shift':-3.5, 'cx':-5, 'cy':0}\n",
    "corrections['subject20_session03'] = {'fs':174, 'tg':'base', 'shift':-4, 'cx':-5, 'cy':5}\n",
    "\n",
    "corrections['subject21_session01'] = {'fs':200, 'tg':'base', 'shift':-3, 'cx':0, 'cy':0}\n",
    "corrections['subject21_session02'] = {'fs':200, 'tg':'base', 'shift':-3.5, 'cx':2, 'cy':-2}\n",
    "corrections['subject21_session03'] = {'fs':200, 'tg':'base', 'shift':-2.8, 'cx':5, 'cy':5}\n",
    "\n",
    "corrections['subject02_session03'] = {'fs':200, 'tg':'base', 'shift':1.0, 'cx':-3, 'cy':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded! 63 Sessions\n",
      "Done sorting trials!\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/enea/Dropbox/Capocaccia2019_Gesture_DVS_Myo/Dataset/EMG/'\n",
    "VERBOSE = False\n",
    "subjects = {}\n",
    "names = sorted([name for name in os.listdir(data_dir) if \"emg\" in name])\n",
    "for name in names:\n",
    "    _emg = np.load(data_dir + '{}'.format(name)).astype('float32')\n",
    "    _ann = np.concatenate([np.array(['none']), np.load(data_dir + '{}'.format(name.replace(\"emg\",\"ann\")))[:-1]])\n",
    "\n",
    "    subjects[\"_\".join(name.split(\"_\")[:2])] = Person(name.split(\"_\")[0], _emg, _ann, classes=classes)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"Loaded {}: EMG = [{}] // ANN = [{}]\".format(\"_\".join(name.split(\"_\")[:2]), _emg.shape, len(_ann)))\n",
    "print(\"Data Loaded! {} Sessions\".format(len(subjects.keys())))\n",
    "\n",
    "# separates data in correct trial type\n",
    "for name, data in subjects.items():\n",
    "    for _class in classes:\n",
    "        _annotation = np.float32(data.ann == _class)\n",
    "        derivative = np.diff(_annotation)/1.0\n",
    "        begins = np.where(derivative == 1)[0]\n",
    "        ends = np.where(derivative == -1)[0]\n",
    "        for b, e in zip(begins, ends):\n",
    "            _trials = data.emg[b:e]\n",
    "            data.trials[_class].append(_trials)\n",
    "            data.begs[_class].append(b)\n",
    "            data.ends[_class].append(e)\n",
    "print(\"Done sorting trials!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1575\n",
      "1575\n",
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[1, 2, 3]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "X_EMG = []\n",
    "Y_EMG = []\n",
    "SUB_EMG = []\n",
    "SES_EMG = []\n",
    "TRI_EMG = []\n",
    "\n",
    "for name, data in subjects.items():\n",
    "    for gesture in classes:\n",
    "        for trial in range(5):\n",
    "            X_EMG.append(data.trials[gesture][trial])\n",
    "            Y_EMG.append(classes_dict[gesture])\n",
    "            SUB_EMG.append(int(name[7:9]))\n",
    "            SES_EMG.append(int(name[17:19]))\n",
    "            TRI_EMG.append(trial)\n",
    "\n",
    "X_EMG = np.array(X_EMG)\n",
    "Y_EMG = np.array(Y_EMG)\n",
    "SUB_EMG = np.array(SUB_EMG)\n",
    "SES_EMG = np.array(SES_EMG)\n",
    "TRI_EMG = np.array(TRI_EMG)\n",
    "print(len(X_EMG))\n",
    "print(len(Y_EMG))\n",
    "print(list(set(Y_EMG)))\n",
    "print(list(set(SUB_EMG)))\n",
    "print(list(set(SES_EMG)))\n",
    "print(list(set(TRI_EMG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28782, 16)\n",
      "(28782, 40, 40, 3)\n",
      "(28782,)\n",
      "(28782,)\n",
      "(28782,)\n"
     ]
    }
   ],
   "source": [
    "frame_len = 0.2\n",
    "frame_step = 0.1\n",
    "F_SUB = []\n",
    "F_SESS = []\n",
    "F_Y = []\n",
    "F_IMG = []\n",
    "F_EMG = []\n",
    "\n",
    "# CREATE ACTUAL DATASET\n",
    "for subject in range(1, 22):\n",
    "    for session in range(1, 4):\n",
    "        for gesture in range(5):\n",
    "            for trial in range(5):\n",
    "                \n",
    "                fs = corrections['subject{:02}_session0{}'.format(subject, session)]['fs']\n",
    "                \n",
    "                idx_emg = np.logical_and.reduce([SUB_EMG == subject, \n",
    "                                                 SES_EMG == session, \n",
    "                                                 TRI_EMG == trial, \n",
    "                                                 Y_EMG == gesture])\n",
    "                idx_img = np.logical_and.reduce([SUB_IMG == subject, \n",
    "                                                 SES_IMG == session, \n",
    "                                                 TRI_IMG == trial, \n",
    "                                                 Y_IMG == gesture])\n",
    "                \n",
    "                e = X_EMG[idx_emg][0]\n",
    "                f = X_IMG[idx_img]\n",
    "                \n",
    "                mav = analyze(e, frame_len=0.2, frame_step=0.1, feat='MSV', preprocess=False)\n",
    "                rms = analyze(e, frame_len=0.2, frame_step=0.1, feat='RMS', preprocess=False)\n",
    "                a = np.concatenate([mav, rms], 1)\n",
    "                \n",
    "                mapping = np.arange(len(a)) * len(f) // len(a)\n",
    "                F_EMG.append(a)\n",
    "                F_IMG.append(np.stack(f[mapping]))\n",
    "                F_SUB.append(np.ones((len(mapping))) * subject)\n",
    "                F_SESS.append(np.ones((len(mapping))) * session)\n",
    "                F_Y.append(np.ones((len(mapping))) * gesture)\n",
    "\n",
    "F_EMG = np.vstack(F_EMG)\n",
    "F_IMG = np.vstack(F_IMG)\n",
    "F_SUB = np.hstack(F_SUB)\n",
    "F_SESS = np.hstack(F_SESS)\n",
    "F_Y = np.hstack(F_Y)\n",
    "\n",
    "print(F_EMG.shape)\n",
    "print(F_IMG.shape)\n",
    "print(F_SUB.shape)\n",
    "print(F_SESS.shape)\n",
    "print(F_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject)\n",
    "print(session)\n",
    "print(gesture)\n",
    "print(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19220, 16)\n",
      "(19220, 40, 40, 3)\n",
      "(19220, 5)\n",
      "(9562, 16)\n",
      "(9562, 40, 40, 3)\n",
      "(9562, 5)\n"
     ]
    }
   ],
   "source": [
    "test_ses = 1\n",
    "\n",
    "x_emg_train = F_EMG[F_SESS != test_ses].astype('float32')\n",
    "x_emg_test = F_EMG[F_SESS == test_ses].astype('float32')\n",
    "\n",
    "x_img_train = F_IMG[F_SESS != test_ses].astype('float32')\n",
    "x_img_test = F_IMG[F_SESS == test_ses].astype('float32')\n",
    "\n",
    "y_train = F_Y[F_SESS != test_ses]\n",
    "y_test = F_Y[F_SESS == test_ses]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 5)\n",
    "y_test = keras.utils.to_categorical(y_test, 5)\n",
    "\n",
    "print(x_emg_train.shape)\n",
    "print(x_img_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_emg_test.shape)\n",
    "print(x_img_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_min =  0.0\n",
      "img_max =  253.0\n",
      "img_mean =  0.5144386\n",
      "img_std =  0.26730623841285805\n",
      "emg_min =  0.0\n",
      "emg_max =  87.84133\n",
      "emg_mean =  0.073996924\n",
      "emg_std =  0.11713933944702248\n"
     ]
    }
   ],
   "source": [
    "##### img preprocessing\n",
    "# normalize\n",
    "data_max = np.max(x_img_train)\n",
    "data_min = np.min(x_img_train)\n",
    "for i in range(len(x_img_train)):\n",
    "    x_img_train[i] = (x_img_train[i] - data_min) / (data_max - data_min)\n",
    "for i in range(len(x_img_test)):\n",
    "    x_img_test[i] = (x_img_test[i] - data_min) / (data_max - data_min)\n",
    "print(\"img_min = \", data_min)\n",
    "print(\"img_max = \", data_max)\n",
    "\n",
    "# standardize\n",
    "data_mean = np.mean(x_img_train)\n",
    "data_std = np.std(x_img_train) + 1e-15\n",
    "x_img_train -= data_mean\n",
    "x_img_train /= data_std\n",
    "x_img_test -= data_mean\n",
    "x_img_test /= data_std\n",
    "print(\"img_mean = \", data_mean)\n",
    "print(\"img_std = \", data_std)\n",
    "\n",
    "# emg preprocessing\n",
    "# normalize\n",
    "data_max = np.max(x_emg_train)\n",
    "data_min = np.min(x_emg_train)\n",
    "for i in range(len(x_emg_train)):\n",
    "    x_emg_train[i] = (x_emg_train[i] - data_min) / (data_max - data_min)\n",
    "for i in range(len(x_emg_test)):\n",
    "    x_emg_test[i] = (x_emg_test[i] - data_min) / (data_max - data_min)\n",
    "print(\"emg_min = \", data_min)\n",
    "print(\"emg_max = \", data_max)\n",
    "\n",
    "# standardize\n",
    "data_mean = np.mean(x_emg_train)\n",
    "data_std = np.std(x_emg_train) + 1e-15\n",
    "x_emg_train -= data_mean\n",
    "x_emg_train /= data_std\n",
    "x_emg_test -= data_mean\n",
    "x_emg_test /= data_std\n",
    "print(\"emg_mean = \", data_mean)\n",
    "print(\"emg_std = \", data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE CNN (SUMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 38, 38, 8)         224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 19, 19, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 17, 17, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 32)          4640      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 598,933\n",
      "Trainable params: 598,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cnn for img\n",
    "img_input_shape_s = (40, 40, 3)\n",
    "num_classes = 5\n",
    "# create the cnn model\n",
    "model_img_s = Sequential()\n",
    "model_img_s.add(Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=img_input_shape_s))\n",
    "model_img_s.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_img_s.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model_img_s.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_img_s.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model_img_s.add(Dropout(0.25))\n",
    "model_img_s.add(Flatten())\n",
    "model_img_s.add(Dense(512, activation='relu'))\n",
    "model_img_s.add(Dropout(0.5))\n",
    "model_img_s.add(Dense(num_classes, activation='softmax'))\n",
    "                    \n",
    "model_img_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/10\n",
      "19220/19220 [==============================] - 25s 1ms/step - loss: 0.5347 - accuracy: 0.7971 - val_loss: 0.2184 - val_accuracy: 0.9450\n",
      "Epoch 2/10\n",
      "19220/19220 [==============================] - 23s 1ms/step - loss: 0.1312 - accuracy: 0.9614 - val_loss: 0.2017 - val_accuracy: 0.9531\n",
      "Epoch 3/10\n",
      "19220/19220 [==============================] - 24s 1ms/step - loss: 0.0895 - accuracy: 0.9726 - val_loss: 0.2123 - val_accuracy: 0.9527\n",
      "Epoch 4/10\n",
      "19220/19220 [==============================] - 24s 1ms/step - loss: 0.0701 - accuracy: 0.9788 - val_loss: 0.2253 - val_accuracy: 0.9547\n",
      "Epoch 5/10\n",
      "19220/19220 [==============================] - 22s 1ms/step - loss: 0.0562 - accuracy: 0.9824 - val_loss: 0.3402 - val_accuracy: 0.9282\n",
      "Epoch 6/10\n",
      "19220/19220 [==============================] - 22s 1ms/step - loss: 0.0524 - accuracy: 0.9845 - val_loss: 0.2128 - val_accuracy: 0.9562\n",
      "Epoch 7/10\n",
      "19220/19220 [==============================] - 21s 1ms/step - loss: 0.0463 - accuracy: 0.9854 - val_loss: 0.2130 - val_accuracy: 0.9564\n",
      "Epoch 8/10\n",
      "19220/19220 [==============================] - 20s 1ms/step - loss: 0.0373 - accuracy: 0.9889 - val_loss: 0.2307 - val_accuracy: 0.9557\n",
      "Epoch 9/10\n",
      "19220/19220 [==============================] - 21s 1ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.2224 - val_accuracy: 0.9573\n",
      "Epoch 10/10\n",
      "19220/19220 [==============================] - 20s 1ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.2402 - val_accuracy: 0.9560\n",
      "9562/9562 [==============================] - 3s 277us/step\n",
      "accuracy: 95.60%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_img_s.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_img_s.fit(x_img_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=(x_img_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_img_s.evaluate(x_img_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model_img_s.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 19,333\n",
      "Trainable params: 19,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp for emg\n",
    "img_input_shape_s = (16,)\n",
    "# create the cnn model\n",
    "model_emg_s = Sequential()\n",
    "model_emg_s.add(Dense(128, activation='relu', input_shape=img_input_shape_s))\n",
    "model_emg_s.add(Dropout(0.5))\n",
    "model_emg_s.add(Dense(128, activation='relu'))\n",
    "model_emg_s.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_emg_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/30\n",
      "19220/19220 [==============================] - 2s 111us/step - loss: 1.1700 - accuracy: 0.5024 - val_loss: 1.0373 - val_accuracy: 0.5854\n",
      "Epoch 2/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.9496 - accuracy: 0.6002 - val_loss: 0.9986 - val_accuracy: 0.6171\n",
      "Epoch 3/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.8842 - accuracy: 0.6315 - val_loss: 0.9952 - val_accuracy: 0.6108\n",
      "Epoch 4/30\n",
      "19220/19220 [==============================] - 2s 84us/step - loss: 0.8471 - accuracy: 0.6473 - val_loss: 0.9850 - val_accuracy: 0.6190\n",
      "Epoch 5/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.8159 - accuracy: 0.6642 - val_loss: 0.9839 - val_accuracy: 0.6112\n",
      "Epoch 6/30\n",
      "19220/19220 [==============================] - 2s 89us/step - loss: 0.7934 - accuracy: 0.6732 - val_loss: 0.9833 - val_accuracy: 0.6222\n",
      "Epoch 7/30\n",
      "19220/19220 [==============================] - 2s 89us/step - loss: 0.7791 - accuracy: 0.6799 - val_loss: 0.9498 - val_accuracy: 0.6239\n",
      "Epoch 8/30\n",
      "19220/19220 [==============================] - 2s 88us/step - loss: 0.7605 - accuracy: 0.6913 - val_loss: 0.9746 - val_accuracy: 0.6173\n",
      "Epoch 9/30\n",
      "19220/19220 [==============================] - 2s 89us/step - loss: 0.7501 - accuracy: 0.6909 - val_loss: 0.9739 - val_accuracy: 0.6178\n",
      "Epoch 10/30\n",
      "19220/19220 [==============================] - 2s 88us/step - loss: 0.7369 - accuracy: 0.6944 - val_loss: 0.9690 - val_accuracy: 0.6180\n",
      "Epoch 11/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.7318 - accuracy: 0.7004 - val_loss: 0.9699 - val_accuracy: 0.6232\n",
      "Epoch 12/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.7154 - accuracy: 0.7061 - val_loss: 0.9488 - val_accuracy: 0.6353\n",
      "Epoch 13/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.7080 - accuracy: 0.7081 - val_loss: 0.9739 - val_accuracy: 0.6211\n",
      "Epoch 14/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.7019 - accuracy: 0.7160 - val_loss: 0.9922 - val_accuracy: 0.6241\n",
      "Epoch 15/30\n",
      "19220/19220 [==============================] - 2s 82us/step - loss: 0.6941 - accuracy: 0.7181 - val_loss: 0.9343 - val_accuracy: 0.6399\n",
      "Epoch 16/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6874 - accuracy: 0.7195 - val_loss: 0.9565 - val_accuracy: 0.6276\n",
      "Epoch 17/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6756 - accuracy: 0.7226 - val_loss: 0.9675 - val_accuracy: 0.6262\n",
      "Epoch 18/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6808 - accuracy: 0.7214 - val_loss: 0.9450 - val_accuracy: 0.6324\n",
      "Epoch 19/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6621 - accuracy: 0.7323 - val_loss: 0.9307 - val_accuracy: 0.6407\n",
      "Epoch 20/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6572 - accuracy: 0.7316 - val_loss: 0.9389 - val_accuracy: 0.6442\n",
      "Epoch 21/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6625 - accuracy: 0.7307 - val_loss: 0.9440 - val_accuracy: 0.6397\n",
      "Epoch 22/30\n",
      "19220/19220 [==============================] - 2s 81us/step - loss: 0.6525 - accuracy: 0.7317 - val_loss: 0.9699 - val_accuracy: 0.6292\n",
      "Epoch 23/30\n",
      "19220/19220 [==============================] - 2s 82us/step - loss: 0.6438 - accuracy: 0.7406 - val_loss: 0.9410 - val_accuracy: 0.6466\n",
      "Epoch 24/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6475 - accuracy: 0.7371 - val_loss: 0.9323 - val_accuracy: 0.6486\n",
      "Epoch 25/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6383 - accuracy: 0.7406 - val_loss: 0.9198 - val_accuracy: 0.6528\n",
      "Epoch 26/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6362 - accuracy: 0.7414 - val_loss: 0.9451 - val_accuracy: 0.6403\n",
      "Epoch 27/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6354 - accuracy: 0.7424 - val_loss: 0.9589 - val_accuracy: 0.6348\n",
      "Epoch 28/30\n",
      "19220/19220 [==============================] - 2s 80us/step - loss: 0.6232 - accuracy: 0.7488 - val_loss: 0.9702 - val_accuracy: 0.6435\n",
      "Epoch 29/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6220 - accuracy: 0.7469 - val_loss: 0.9521 - val_accuracy: 0.6413\n",
      "Epoch 30/30\n",
      "19220/19220 [==============================] - 2s 79us/step - loss: 0.6162 - accuracy: 0.7511 - val_loss: 0.9743 - val_accuracy: 0.6348\n",
      "accuracy: 63.48%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_emg_s.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_emg_s.fit(x_emg_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=30,\n",
    "        verbose=1,\n",
    "        validation_data=(x_emg_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_emg_s.evaluate(x_emg_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_emg_s.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv2d_16_input (InputLayer)    (None, 40, 40, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 38, 38, 8)    224         conv2d_16_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 19, 19, 8)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 17, 17, 16)   1168        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 8, 8, 16)     0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 6, 6, 32)     4640        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 6, 6, 32)     0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_58_input (InputLayer)     (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1152)         0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 128)          2176        dense_58_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 512)          590336      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 512)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 128)          16512       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 5)            2565        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 5)            645         dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 10)           0           dense_57[0][0]                   \n",
      "                                                                 dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 5)            55          concatenate_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 618,321\n",
      "Trainable params: 618,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mergedOut_s = Concatenate()([model_img_s.output, model_emg_s.output])\n",
    "mergedOut_s = Dense(5, activation='softmax')(mergedOut_s)\n",
    "model_fus_s = Model([model_img_s.input, model_emg_s.input], mergedOut_s)\n",
    "model_fus_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x147e0f898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x147e0f6a0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x147e0f710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x147e0f908> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1197ef908> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x119801668> False\n",
      "<keras.layers.core.Dropout object at 0x119801748> False\n",
      "<keras.engine.input_layer.InputLayer object at 0x152023358> False\n",
      "<keras.layers.core.Flatten object at 0x119820e80> False\n",
      "<keras.layers.core.Dense object at 0x13ab44160> False\n",
      "<keras.layers.core.Dense object at 0x119867588> False\n",
      "<keras.layers.core.Dropout object at 0x147de79b0> False\n",
      "<keras.layers.core.Dropout object at 0x119867470> False\n",
      "<keras.layers.core.Dense object at 0x152023ac8> False\n",
      "<keras.layers.core.Dense object at 0x1198b9e80> False\n",
      "<keras.layers.core.Dense object at 0x152023128> False\n",
      "<keras.layers.merge.Concatenate object at 0x14a282048> False\n",
      "<keras.layers.core.Dense object at 0x15205ec88> True\n"
     ]
    }
   ],
   "source": [
    "# freeze the layers except the last dense\n",
    "for layer in model_fus_s.layers[:len(model_fus_s.layers)-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# check the status of the layers\n",
    "for layer in model_fus_s.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/10\n",
      "19220/19220 [==============================] - 9s 457us/step - loss: 0.8500 - accuracy: 0.8236 - val_loss: 0.4919 - val_accuracy: 0.9569\n",
      "Epoch 2/10\n",
      "19220/19220 [==============================] - 8s 432us/step - loss: 0.2336 - accuracy: 0.9922 - val_loss: 0.2285 - val_accuracy: 0.9624\n",
      "Epoch 3/10\n",
      "19220/19220 [==============================] - 8s 436us/step - loss: 0.0878 - accuracy: 0.9934 - val_loss: 0.1636 - val_accuracy: 0.9612\n",
      "Epoch 4/10\n",
      "19220/19220 [==============================] - 8s 431us/step - loss: 0.0468 - accuracy: 0.9944 - val_loss: 0.1499 - val_accuracy: 0.9601\n",
      "Epoch 5/10\n",
      "19220/19220 [==============================] - 8s 433us/step - loss: 0.0358 - accuracy: 0.9931 - val_loss: 0.1478 - val_accuracy: 0.9601\n",
      "Epoch 6/10\n",
      "19220/19220 [==============================] - 8s 436us/step - loss: 0.0309 - accuracy: 0.9930 - val_loss: 0.1488 - val_accuracy: 0.9598\n",
      "Epoch 7/10\n",
      "19220/19220 [==============================] - 8s 430us/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.1507 - val_accuracy: 0.9598\n",
      "Epoch 8/10\n",
      "19220/19220 [==============================] - 8s 432us/step - loss: 0.0274 - accuracy: 0.9932 - val_loss: 0.1517 - val_accuracy: 0.9599\n",
      "Epoch 9/10\n",
      "19220/19220 [==============================] - 8s 431us/step - loss: 0.0268 - accuracy: 0.9930 - val_loss: 0.1525 - val_accuracy: 0.9596\n",
      "Epoch 10/10\n",
      "19220/19220 [==============================] - 9s 455us/step - loss: 0.0230 - accuracy: 0.9938 - val_loss: 0.1543 - val_accuracy: 0.9596\n",
      "9562/9562 [==============================] - 3s 289us/step\n",
      "accuracy: 95.96%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_fus_s.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_fus_s.fit([x_img_train, x_emg_train], y_train,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=([x_img_test, x_emg_test], y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_fus_s.evaluate([x_img_test, x_emg_test], y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model_fus_s.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emg_s.save('model_emg_s.h5') \n",
    "model_img_s.save('model_img_s.h5') \n",
    "model_fus_s.save('model_fus_s.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MLP (CHARLOTTE) [C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19220, 1600)\n",
      "(9562, 1600)\n"
     ]
    }
   ],
   "source": [
    "x_img_train_c = x_img_train[:, :, :, 0].reshape(-1, 1600)\n",
    "x_img_test_c = x_img_test[:, :, :, 0].reshape(-1, 1600)\n",
    "print(x_img_train_c.shape)\n",
    "print(x_img_test_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 210)               336210    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 210)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 5)                 1055      \n",
      "=================================================================\n",
      "Total params: 337,265\n",
      "Trainable params: 337,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp for img\n",
    "img_input_shape_c = (1600,)\n",
    "# create the mlp model\n",
    "model_img_c = Sequential()\n",
    "model_img_c.add(Dense(210, activation='relu', input_shape=img_input_shape_c))\n",
    "model_img_c.add(Dropout(0.3))\n",
    "model_img_c.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_img_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/10\n",
      "19220/19220 [==============================] - 5s 256us/step - loss: 1.4201 - accuracy: 0.4158 - val_loss: 1.1334 - val_accuracy: 0.5059\n",
      "Epoch 2/10\n",
      "19220/19220 [==============================] - 4s 228us/step - loss: 1.0713 - accuracy: 0.5393 - val_loss: 1.0625 - val_accuracy: 0.5451\n",
      "Epoch 3/10\n",
      "19220/19220 [==============================] - 4s 218us/step - loss: 0.9486 - accuracy: 0.5924 - val_loss: 0.9239 - val_accuracy: 0.6063\n",
      "Epoch 4/10\n",
      "19220/19220 [==============================] - 4s 214us/step - loss: 0.8838 - accuracy: 0.6196 - val_loss: 1.0926 - val_accuracy: 0.5789\n",
      "Epoch 5/10\n",
      "19220/19220 [==============================] - 4s 215us/step - loss: 0.8240 - accuracy: 0.6450 - val_loss: 0.8998 - val_accuracy: 0.6287\n",
      "Epoch 6/10\n",
      "19220/19220 [==============================] - 4s 223us/step - loss: 0.7856 - accuracy: 0.6626 - val_loss: 0.8752 - val_accuracy: 0.6398\n",
      "Epoch 7/10\n",
      "19220/19220 [==============================] - 4s 222us/step - loss: 0.7465 - accuracy: 0.6822 - val_loss: 0.9884 - val_accuracy: 0.6241\n",
      "Epoch 8/10\n",
      "19220/19220 [==============================] - 4s 217us/step - loss: 0.7227 - accuracy: 0.6966 - val_loss: 0.8632 - val_accuracy: 0.6607\n",
      "Epoch 9/10\n",
      "19220/19220 [==============================] - 4s 213us/step - loss: 0.6951 - accuracy: 0.7050 - val_loss: 0.8763 - val_accuracy: 0.6519\n",
      "Epoch 10/10\n",
      "19220/19220 [==============================] - 4s 218us/step - loss: 0.6696 - accuracy: 0.7233 - val_loss: 0.8395 - val_accuracy: 0.6681\n",
      "9562/9562 [==============================] - 1s 56us/step\n",
      "accuracy: 66.81%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_img_c.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_img_c.fit(x_img_train_c, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=(x_img_test_c, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_img_c.evaluate(x_img_test_c, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model_img_c.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 230)               3910      \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 5)                 1155      \n",
      "=================================================================\n",
      "Total params: 5,065\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp for emg\n",
    "img_input_shape_c = (16,)\n",
    "# create the cnn model\n",
    "model_emg_c = Sequential()\n",
    "model_emg_c.add(Dense(230, activation='relu', input_shape=img_input_shape_c))\n",
    "model_emg_c.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_emg_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/30\n",
      "19220/19220 [==============================] - 2s 97us/step - loss: 1.0810 - accuracy: 0.5553 - val_loss: 1.0525 - val_accuracy: 0.5699\n",
      "Epoch 2/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.8838 - accuracy: 0.6381 - val_loss: 1.0316 - val_accuracy: 0.5836\n",
      "Epoch 3/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.8185 - accuracy: 0.6669 - val_loss: 0.9901 - val_accuracy: 0.5984\n",
      "Epoch 4/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.7786 - accuracy: 0.6814 - val_loss: 1.0146 - val_accuracy: 0.5934\n",
      "Epoch 5/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.7492 - accuracy: 0.6982 - val_loss: 0.9562 - val_accuracy: 0.6172\n",
      "Epoch 6/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.7243 - accuracy: 0.7107 - val_loss: 0.9937 - val_accuracy: 0.6083\n",
      "Epoch 7/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.7033 - accuracy: 0.7202 - val_loss: 0.9874 - val_accuracy: 0.6066\n",
      "Epoch 8/30\n",
      "19220/19220 [==============================] - 1s 69us/step - loss: 0.6877 - accuracy: 0.7237 - val_loss: 0.9611 - val_accuracy: 0.6219\n",
      "Epoch 9/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.6730 - accuracy: 0.7296 - val_loss: 0.9683 - val_accuracy: 0.6276\n",
      "Epoch 10/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.6605 - accuracy: 0.7356 - val_loss: 0.9855 - val_accuracy: 0.6193\n",
      "Epoch 11/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.6474 - accuracy: 0.7429 - val_loss: 0.9860 - val_accuracy: 0.6232\n",
      "Epoch 12/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.6371 - accuracy: 0.7479 - val_loss: 1.0084 - val_accuracy: 0.6146\n",
      "Epoch 13/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.6270 - accuracy: 0.7517 - val_loss: 0.9870 - val_accuracy: 0.6147\n",
      "Epoch 14/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.6188 - accuracy: 0.7552 - val_loss: 1.0091 - val_accuracy: 0.6206\n",
      "Epoch 15/30\n",
      "19220/19220 [==============================] - 1s 69us/step - loss: 0.6100 - accuracy: 0.7592 - val_loss: 1.0004 - val_accuracy: 0.6182\n",
      "Epoch 16/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.6033 - accuracy: 0.7618 - val_loss: 1.0085 - val_accuracy: 0.6260\n",
      "Epoch 17/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5963 - accuracy: 0.7638 - val_loss: 1.0453 - val_accuracy: 0.6144\n",
      "Epoch 18/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5896 - accuracy: 0.7675 - val_loss: 0.9982 - val_accuracy: 0.6246\n",
      "Epoch 19/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5834 - accuracy: 0.7680 - val_loss: 1.0264 - val_accuracy: 0.6168\n",
      "Epoch 20/30\n",
      "19220/19220 [==============================] - 1s 70us/step - loss: 0.5773 - accuracy: 0.7702 - val_loss: 1.0184 - val_accuracy: 0.6164\n",
      "Epoch 21/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5730 - accuracy: 0.7757 - val_loss: 1.0144 - val_accuracy: 0.6218\n",
      "Epoch 22/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.5676 - accuracy: 0.7754 - val_loss: 1.0243 - val_accuracy: 0.6230\n",
      "Epoch 23/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5632 - accuracy: 0.7776 - val_loss: 1.0037 - val_accuracy: 0.6245\n",
      "Epoch 24/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5575 - accuracy: 0.7766 - val_loss: 1.0958 - val_accuracy: 0.5969\n",
      "Epoch 25/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.5541 - accuracy: 0.7811 - val_loss: 1.0145 - val_accuracy: 0.6174\n",
      "Epoch 26/30\n",
      "19220/19220 [==============================] - 1s 67us/step - loss: 0.5493 - accuracy: 0.7852 - val_loss: 1.0312 - val_accuracy: 0.6233\n",
      "Epoch 27/30\n",
      "19220/19220 [==============================] - 1s 68us/step - loss: 0.5475 - accuracy: 0.7820 - val_loss: 1.0244 - val_accuracy: 0.6192\n",
      "Epoch 28/30\n",
      "19220/19220 [==============================] - 1s 69us/step - loss: 0.5413 - accuracy: 0.7870 - val_loss: 1.0297 - val_accuracy: 0.6274\n",
      "Epoch 29/30\n",
      "19220/19220 [==============================] - 1s 69us/step - loss: 0.5389 - accuracy: 0.7881 - val_loss: 1.0240 - val_accuracy: 0.6285\n",
      "Epoch 30/30\n",
      "19220/19220 [==============================] - 1s 69us/step - loss: 0.5356 - accuracy: 0.7887 - val_loss: 1.0505 - val_accuracy: 0.6265\n",
      "accuracy: 62.65%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_emg_c.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_emg_c.fit(x_emg_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=30,\n",
    "        verbose=1,\n",
    "        validation_data=(x_emg_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_emg_c.evaluate(x_emg_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_emg_c.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dense_62_input (InputLayer)     (None, 1600)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 210)          336210      dense_62_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_64_input (InputLayer)     (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 210)          0           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 230)          3910        dense_64_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 5)            1055        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 5)            1155        dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 10)           0           dense_63[0][0]                   \n",
      "                                                                 dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 5)            55          concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 342,385\n",
      "Trainable params: 342,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mergedOut_c = Concatenate()([model_img_c.output, model_emg_c.output])\n",
    "mergedOut_c = Dense(5, activation='softmax')(mergedOut_c)\n",
    "model_fus_c = Model([model_img_c.input, model_emg_c.input], mergedOut_c)\n",
    "model_fus_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x15300fc88> False\n",
      "<keras.layers.core.Dense object at 0x15300f860> False\n",
      "<keras.engine.input_layer.InputLayer object at 0x1514484a8> False\n",
      "<keras.layers.core.Dropout object at 0x15300fe48> False\n",
      "<keras.layers.core.Dense object at 0x152fc6240> False\n",
      "<keras.layers.core.Dense object at 0x15300fcf8> False\n",
      "<keras.layers.core.Dense object at 0x152fc6518> False\n",
      "<keras.layers.merge.Concatenate object at 0x150da04e0> False\n",
      "<keras.layers.core.Dense object at 0x150da04a8> True\n"
     ]
    }
   ],
   "source": [
    "# freeze the layers except the last dense\n",
    "for layer in model_fus_c.layers[:len(model_fus_c.layers)-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# check the status of the layers\n",
    "for layer in model_fus_c.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19220 samples, validate on 9562 samples\n",
      "Epoch 1/10\n",
      "19220/19220 [==============================] - 3s 139us/step - loss: 1.0762 - accuracy: 0.7441 - val_loss: 0.8576 - val_accuracy: 0.7965\n",
      "Epoch 2/10\n",
      "19220/19220 [==============================] - 2s 109us/step - loss: 0.5315 - accuracy: 0.9088 - val_loss: 0.6189 - val_accuracy: 0.8127\n",
      "Epoch 3/10\n",
      "19220/19220 [==============================] - 2s 110us/step - loss: 0.3632 - accuracy: 0.9104 - val_loss: 0.5455 - val_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "19220/19220 [==============================] - 2s 113us/step - loss: 0.3068 - accuracy: 0.9129 - val_loss: 0.5242 - val_accuracy: 0.8149\n",
      "Epoch 5/10\n",
      "19220/19220 [==============================] - 2s 113us/step - loss: 0.2784 - accuracy: 0.9134 - val_loss: 0.5169 - val_accuracy: 0.8156\n",
      "Epoch 6/10\n",
      "19220/19220 [==============================] - 2s 111us/step - loss: 0.2648 - accuracy: 0.9132 - val_loss: 0.5156 - val_accuracy: 0.8163\n",
      "Epoch 7/10\n",
      "19220/19220 [==============================] - 2s 112us/step - loss: 0.2615 - accuracy: 0.9142 - val_loss: 0.5155 - val_accuracy: 0.8166\n",
      "Epoch 8/10\n",
      "19220/19220 [==============================] - 2s 109us/step - loss: 0.2573 - accuracy: 0.9136 - val_loss: 0.5154 - val_accuracy: 0.8177\n",
      "Epoch 9/10\n",
      "19220/19220 [==============================] - 2s 129us/step - loss: 0.2522 - accuracy: 0.9133 - val_loss: 0.5171 - val_accuracy: 0.8178\n",
      "Epoch 10/10\n",
      "19220/19220 [==============================] - 2s 119us/step - loss: 0.2475 - accuracy: 0.9146 - val_loss: 0.5182 - val_accuracy: 0.8176\n",
      "9562/9562 [==============================] - 1s 115us/step\n",
      "accuracy: 81.76%\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model_fus_c.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_fus_c.fit([x_img_train_c, x_emg_train], y_train,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=([x_img_test_c, x_emg_test], y_test))\n",
    "\n",
    "# evaluate the model\n",
    "scores = model_fus_c.evaluate([x_img_test_c, x_emg_test], y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model_fus_c.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emg_c.save('model_emg_c.h5') \n",
    "model_img_c.save('model_img_c.h5') \n",
    "model_fus_c.save('model_fus_c.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortex-A53 (ARMv8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14a22a2b0>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpu_usages = np.load('/Users/enea/profile_fus_cpu_usage.npy')\n",
    "plt.plot(cpu_usages[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.5909090909091\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cpu_usages[1000:][cpu_usages[1000:] > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83.   94.2  93.8  93.8  93.8  93.8  93.6  93.5  93.5  93.5  93.6  93.6\n",
      "  93.6  93.6  93.7  92.8  94.1  93.8  93.7  93.6  93.7  93.5  93.5  93.7\n",
      "  93.7 102.9  56.6  45.   58.2  94.6  65.1  43.9  43.9  88.4  44.1  44.2\n",
      "  53.3  45.   88.5  77.1  44.3  44.2  87.8  44.2]\n"
     ]
    }
   ],
   "source": [
    "print(cpu_usages[1000:][cpu_usages[1000:] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'feat', 'frame_len', 'test_ses', 'accuracy'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./full_results_cross_latency_v1.csv')\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                frame_len  test_ses   accuracy\n",
      "model     feat                                \n",
      "charlotte emg         0.2         2  65.363757\n",
      "          fus         0.2         2  82.117099\n",
      "          img         0.2         2  64.151708\n",
      "sumit     emg         0.2         2  68.071151\n",
      "          fus         0.2         2  95.733442\n",
      "          img         0.2         2  95.197785\n",
      "                frame_len  test_ses  accuracy\n",
      "model     feat                               \n",
      "charlotte emg         0.0       1.0  2.977906\n",
      "          fus         0.0       1.0  3.142194\n",
      "          img         0.0       1.0  5.190006\n",
      "sumit     emg         0.0       1.0  2.848121\n",
      "          fus         0.0       1.0  1.119447\n",
      "          img         0.0       1.0  1.329271\n"
     ]
    }
   ],
   "source": [
    "print(data[data['frame_len'] == 0.2].groupby(['model', 'feat']).mean())\n",
    "print(data[data['frame_len'] == 0.2].groupby(['model', 'feat']).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14f83f320>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=data[data['model'] == 'sumit'], x='frame_len', y='accuracy', hue='feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x150a812b0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=data[data['model'] == 'charlotte'], x='frame_len', y='accuracy', hue='feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvsemg",
   "language": "python",
   "name": "dvsemg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
